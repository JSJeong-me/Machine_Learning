{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1. <a href=\"#load\"> Loading data </a>\n2. <a href=\"#eda\">Exploratory data analysis</a>\n3. <a href=\"#proc\">Pre-processing</a>\n4. <a href=\"#prep\">Data preparation</a>\n    * <a href=\"#tts\">train-test split / LabelEncoder</a>\n    * <a href=\"#scal\">Scaler</a>\n5. <a href=\"#modl\">Build Model</a>\n6. <a href=\"#eval\">Evaluate Model</a>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"load\"> Loading data </a>","metadata":{}},{"cell_type":"code","source":"file = \"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\"\ndf = pd.read_csv(file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"eda\">Exploratory Data Analysis</a>","metadata":{}},{"cell_type":"code","source":"print(df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking linear correlation among features:","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df, hue=\"quality\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = np.zeros_like(df.corr())\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(18,8))\nsns.heatmap(df.corr(), cmap='viridis', mask=mask, annot=False, square=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems there are some high correlated features in dataset, mainly among the following features (their correlations are all above 0.6):\n\n* 'citric acid', 'density' and 'pH' to 'fixed acidity'\n* 'citric acid' to 'volatile acidity'\n* 'total sulfur dioxide' to 'free sulfur dioxide'.\n\nLet's now check correlations to the target (_quality_ feature) and its distribution:","metadata":{}},{"cell_type":"code","source":"# check correlations above 0.6 for fixed acidity', 'volatile acidity' and 'total sulfur dioxide' features\n\n(abs(df.corr()[['fixed acidity', 'volatile acidity', 'total sulfur dioxide']])>0.6)*1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr()['quality'].iloc[:-1].sort_values().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(df.quality)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.quality.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"clearly imbalanced dataset... Maybe we could think about applying SMOTE or some cost-sensitive learning technique. https://machinelearningmastery.com/multi-class-imbalanced-classification/","metadata":{}},{"cell_type":"markdown","source":"now, let's scale all the data, aiming at checking a boxplot of all features, so we can easily see its variability.\n\nWARNING: note that this scaling will be used only for this observation purpose! do not use this specific scaler for posterior training purposes, as you may incur in data snooping.","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(df))\nX_train.columns = df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_theme(style=\"ticks\", palette=\"pastel\")\n\nplt.figure(figsize=(18,8))\nsns.boxplot(data=X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"proc\">Pre-processing</a>","metadata":{}},{"cell_type":"markdown","source":"Based on our previous data exploration, let's drop some high correlated features:","metadata":{}},{"cell_type":"code","source":"# dropping 'citric acid', 'density', 'pH', 'total sulfur dioxide':\n\ndf = df.drop(columns=['citric acid', 'density', 'pH', 'total sulfur dioxide'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"prep\">Data preparation</a>","metadata":{}},{"cell_type":"markdown","source":"Data preparation steps:\n* train-test split\n* MinMax scaling","metadata":{}},{"cell_type":"markdown","source":"### <a id=\"tts\">train-test split / LabelEncoder</a>","metadata":{}},{"cell_type":"code","source":"X = df.loc[:, df.columns != 'quality'].values\ny = df.quality.values\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ny_train_cat = to_categorical(y_train, 6)\ny_test_cat = to_categorical(y_test, 6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"scal\">Scaler</a>","metadata":{}},{"cell_type":"code","source":"scaler = RobustScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"modl\">Build Model</a>","metadata":{}},{"cell_type":"code","source":"xavier_init = tf.keras.initializers.GlorotNormal()\n\nmodel = Sequential()\nmodel.add(Dense(64, kernel_initializer=xavier_init,  activation='relu'))\nmodel.add(Dense(32, kernel_initializer=xavier_init, activation='relu'))\nmodel.add(Dense(16, kernel_initializer=xavier_init, activation='relu'))\nmodel.add(Dense(6, kernel_initializer=xavier_init, activation='sigmoid'))\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train,\n          y_train_cat,\n          epochs=30,\n          validation_data=(X_test,y_test_cat),\n          verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses = pd.DataFrame(model.history.history)\n\nlosses[['loss','val_loss']].plot()\nlosses[['accuracy','val_accuracy']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id=\"eval\">Evaluate Model</a>","metadata":{}},{"cell_type":"code","source":"print(model.metrics_names)\nprint(model.evaluate(X_test,y_test_cat,verbose=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = le.inverse_transform(np.argmax(model.predict(X_test), axis=-1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(le.inverse_transform(y_test),\n                            predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CM = confusion_matrix(le.inverse_transform(y_test), predictions)\nCM","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}